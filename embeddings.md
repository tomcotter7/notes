# Encoder Models

## Finetuning / Training Models

### augmented-SBERT

This [paper](https://arxiv.org/pdf/2010.08240) details a powerful method of finetuning bi-encoders. Essentially, they create a labelled dataset using a cross-encoder, which then can be used to fine-tune a bi-encoder.

Cross-Encoders can be finetuned with less data as well, so this is the typical method for adapting the retrieval models to a specific domain.

### Semantic Re-Tuning with Constrastive Tension

[Paper](https://openreview.net/pdf?id=Ov_sMNau-PF)

This is an self-supervised method for finetuning, and when combining CT with supervised data they found that it outperformed supervised-only methods. 

Two independent models, with identical initialized weights, are set to maximise the dot product between their sentence representations for identical sentences, and minimize the dot product for their sentence representations of differing sentences.

"we find little reason to believe that the realignment enforced by CT to be beneficial for fine-tuning tasks where ample training data is available." -> i.e. it's useful when you don't have much data.

### GPL: Generative Pseudo Labeling for the Unsupervised Doman Adaptation of Dense Retrieval

[Paper](https://arxiv.org/pdf/2112.07577)

Dense retrieval models are extremely sensitive to domain shifts. They train a dense retriever with pairs with similarity scores (which are generated by a cross-encoder).

The dataset format was $(Q, P_{pos}, P_{neg}, M)$, where $M$ is the 'margin' between the cross-encoder score of (Q, P_{pos}) and (Q, P_{neg}). They then use this to train a dense retriever. They used MarginMSE loss to train the dense retriever.

They also have a finetuned generator model to generate the synthetic queries, although I'm not sure how much benefit this would have over something like Claude. 

They use MarginMSE loss with the cross-encoder labels to prevent errors from the query generator (i.e generating queries that are not answerable by the passage). This makes it more robust to badly generated queries.

### Text and Code Embeddings by Contrastive Pre-Training

[Paper](https://arxiv.org/pdf/2201.10005)

In the paper, they show that 'contrastive' pre-training (i.e training to get closer to positives and further away from negatives) leads to high quality embeddings. This can be done unsupervised, as long as you have $A$nchor and $P$ositive pairs, [$A_{1}, P_{1}$], [$A_{2}, P_{2}$], etc. as you can build the negative pairs by doing [$A_{1}, P_{2}$], [$A_{1}, P_{3}$], etc.

Typically, you'll want to do hard negative mining but pseudo-random works somewhat. I have found this to be applicable to cross-encoder models too.

### Finetuning with Modal

[See this article](https://modal.com/blog/fine-tuning-embeddings) for details on using the modal platform to finetune embeddings

### GISTEmbed

This [paper](https://arxiv.org/pdf/2402.16829) details a method on collecting negatives for text embedding fine-tuning. It uses a guide model to enhance in-batch negative selection.

They use large & high performing embedding models to finetune smaller embedding models. So, given a (Q, P) pair, they sample the entire batch for negatives, then use the guide model to filter out any "negatives" with a higher similarity to Q than P. This means you can finetune a model with just (Q, P) pairs, and not have to worry about collecting negatives.

## Evaluation

### Evaluating for IR

[Article](https://ar5iv.labs.arxiv.org/html/2305.06300).

They found that BM25 outperforms embedding models for the re-ranking use case - which makes sense. The best performance came from combining the Cohere **embedding** model with BM25. Note that they are using bi-encoder embedding models for re-ranking, not cross-encoders.

The metrics they used were:
- [nDCG@k](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) - Normalized Discounted Cumulative Gain, k is the number of documents to consider - i.e. the top-k documents. The also looked at recall - which is useful when you have multiple "correct/relevant" answers.

## Loss Functions

### RankedListLoss for Deep Metric Learning

[Paper](https://arxiv.org/pdf/1903.03238)

They argue that existing pairwise or triplet loss functions will suffer from slow converagence to due to a large proproration of trivial pairs / triplets as the model improves.

## Reranking 

### ColBERT

[HuggingFace](https://huggingface.co/colbert-ir/colbertv2.0)
[Demo](https://www.youtube.com/watch?v=cN6S0Ehm7_8)

ColBERT is an alternative to an cross-encoder. Cross-Encoder is an "early interaction" model, it takes in a query & a document, combines them and then outputs a similarity score. This can get pretty slow due to the attention mechanism.

ColBERT is a "late interaction" model. For both the query and the document, you produce an embedding for each **token**. For each token vector in the query, you get the max similarity score across all token vectors in the document. The final score is the sum of all these max similarities.

This is different to a cross-encoder because the q & d are not combined. ColBERT is more efficient because the document embeddings only need to be calculate once. Therefore, if you are reranking over many documents it may be better to use ColBERT.

[JinaAI Article](https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/) - also useful.

### Any* Embedding Model Can Be a Late Interaction Model

[Article](https://qdrant.tech/articles/late-interaction-models/)

In embedding models, the final output is a single vector. However, this vector is produced by a pooling operation (typically mean pooling) of the final representation of the token vectors. 

These token vectors are pretty similar to the way the ColBERT works. This article showed that using these as multi-vector retrieval models actually outperformed ColBERT.

The only issue is that using these multi-vector models with all the output vectors uses up more memory than ColBERT embeddings. However, the authors of the article showed that in this case, quantization to int8 was fine. 

### Late Chunking

Late Chunking is slightly different to Late Interaction defined by ColBERT. Late Interaction compares every token in the query to every token in the document. This can get costly (imagine storing an embedding for each token).

Late Chunking uses Pooling similar to embedding models. Embedding models, would chunk into 512 token chunks, then embed those tokens, and then produce a vector via pooling to a produce a single representation for that entire chunk. But what if we want to span multiple chunks?

Late Chunking would first embed the entire chunk (therefore retaining the attention scores across all tokens). This obtains token embeddings. We split these into a determined sized (512) and then pool the token embeddings after that. This allows us to span multiple chunks.



