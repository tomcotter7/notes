# Entropy

## Shannon Entropy

The entropy of a random variable quantifies the average level of uncertainty or information associated with the variable's possible outcome. Essentially, it measures the expected amount of information needed to describe the state of the variable.

$H(X) = -\sum_{x \in X} p(x) log p(x)$

so $x$ is all possible outcomes of the random variable $X$ and $p(x)$ is the probability of $x$.

Shannon entropy uses a log of base 2.

## Information Content

The information content of an event (suprisal) is a function that increases as the probability of the event decreases.

Entropy measures the average amount of information conveyed by identifying the outcome of a random variable.
